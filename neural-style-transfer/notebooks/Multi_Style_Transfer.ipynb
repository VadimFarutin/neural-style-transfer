{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-Style-Transfer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VadimFarutin/neural-style-transfer/blob/neuro-template/neural-style-transfer/notebooks/Multi_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMUnUBIjpJ3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tnrange, tqdm_notebook\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import copy\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lCFopBnBAI6",
        "colab_type": "code",
        "outputId": "fbba31ca-aa08-452a-b33a-ac0a3e8d8294",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWhRnyTkBAZd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6bdc16d3-44a1-42dd-a228-cbd7a3132e40"
      },
      "source": [
        "cnn = models.vgg19(pretrained=True).features.to(device).eval()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:18<00:00, 31.2MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PcGvkxbBAXX",
        "colab_type": "code",
        "outputId": "fa26d089-174c-46f0-8d56-573bdac5cf3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "!wget \"http://images.cocodataset.org/zips/train2014.zip\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-24 22:37:49--  http://images.cocodataset.org/zips/train2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.10.35\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.10.35|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13510573713 (13G) [application/zip]\n",
            "Saving to: ‘train2014.zip’\n",
            "\n",
            "train2014.zip       100%[===================>]  12.58G  46.6MB/s    in 5m 10s  \n",
            "\n",
            "2019-12-24 22:42:59 (41.5 MB/s) - ‘train2014.zip’ saved [13510573713/13510573713]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUzvR6T5BAVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"train2014.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"train2014\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJhV-JEoA2pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageLoader():\n",
        "    IMAGE_SIZE = 128 if torch.cuda.is_available() else 128\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loader = transforms.Compose([\n",
        "            transforms.Resize((ImageLoader.IMAGE_SIZE, ImageLoader.IMAGE_SIZE)),\n",
        "            transforms.ToTensor()]\n",
        "        )\n",
        "\n",
        "    def load(self, path):\n",
        "        image = Image.open(path)\n",
        "        image = self.loader(image).unsqueeze(0)\n",
        "        image = image.to(device, torch.float)\n",
        "        return image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx0pS11KA5mm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(tensor, title):\n",
        "    unloader = transforms.ToPILImage()\n",
        "\n",
        "    image = tensor.cpu().clone()\n",
        "    image = image.squeeze(0)\n",
        "    image = unloader(image)\n",
        "    plt.figure()\n",
        "    plt.imshow(image)\n",
        "    plt.xticks([]); plt.yticks([]);\n",
        "    plt.title(title)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpIaw8r1A5yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self, target):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.target = target.detach()\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.loss = F.mse_loss(input, self.target)\n",
        "        return input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqXfy9CGA5wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StyleLoss(nn.Module):\n",
        "    def gram_matrix(input):\n",
        "        b, c, h, w = input.size()\n",
        "        features = input.view(b * c, h * w) \n",
        "        G = torch.mm(features, features.t())\n",
        "\n",
        "        return G.div(b * c * h * w)\n",
        "\n",
        "    def __init__(self, target_feature):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.target = StyleLoss.gram_matrix(target_feature).detach()\n",
        "\n",
        "    def forward(self, input):\n",
        "        G = StyleLoss.gram_matrix(input)\n",
        "        self.loss = F.mse_loss(G, self.target)\n",
        "        return input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g7YnL5QBNt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Normalization(nn.Module):\n",
        "    MEAN = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "    STD = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "\n",
        "    def __init__(self, mean=None, std=None):\n",
        "        super(Normalization, self).__init__()\n",
        "\n",
        "        if mean is None:\n",
        "            mean = Normalization.MEAN\n",
        "        if std is None:\n",
        "            std = Normalization.STD\n",
        "\n",
        "        self.mean = mean.clone().detach().view(-1, 1, 1)\n",
        "        self.std = std.clone().detach().view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return (img - self.mean) / self.std\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LafILSbXBN7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, cnn, style_img, content_img, content_layers, style_layers):\n",
        "        super(BaseModel, self).__init__()\n",
        "\n",
        "        cnn = copy.deepcopy(cnn)\n",
        "        normalization = Normalization().to(device)\n",
        "        model = nn.Sequential(normalization)\n",
        "\n",
        "        content_losses = []\n",
        "        style_losses = []\n",
        "\n",
        "        last_loss_layer = 0\n",
        "        conv_cnt = 0\n",
        "\n",
        "        for layer in cnn.children():\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                conv_cnt += 1\n",
        "                name = 'conv_{}'.format(conv_cnt)\n",
        "            elif isinstance(layer, nn.ReLU):\n",
        "                name = 'relu_{}'.format(conv_cnt)\n",
        "                layer = nn.ReLU(inplace=False)\n",
        "            elif isinstance(layer, nn.MaxPool2d):\n",
        "                name = 'pool_{}'.format(conv_cnt)\n",
        "            elif isinstance(layer, nn.BatchNorm2d):\n",
        "                name = 'bn_{}'.format(conv_cnt)\n",
        "            else:\n",
        "                name = 'unknown_{}'.format(conv_cnt)\n",
        "                \n",
        "            model.add_module(name, layer)\n",
        "\n",
        "            if name in content_layers:\n",
        "                target = model(content_img).detach()\n",
        "                content_loss = ContentLoss(target)\n",
        "                model.add_module(\"content_loss_{}\".format(conv_cnt), content_loss)\n",
        "                content_losses.append(content_loss)\n",
        "                last_loss_layer = len(model)\n",
        "\n",
        "            if name in style_layers:\n",
        "                target_feature = model(style_img).detach()\n",
        "                style_loss = StyleLoss(target_feature)\n",
        "                model.add_module(\"style_loss_{}\".format(conv_cnt), style_loss)\n",
        "                style_losses.append(style_loss)\n",
        "                last_loss_layer = len(model)\n",
        "\n",
        "        model = model[:(last_loss_layer)]\n",
        "\n",
        "        self.model = model\n",
        "        self.content_losses = content_losses\n",
        "        self.style_losses = style_losses\n",
        "\n",
        "    def get_content_losses(self, ):\n",
        "        return self.content_losses\n",
        "\n",
        "    def get_style_losses(self, ):\n",
        "        return self.style_losses\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtrrIo4Srx9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GramMatrix(nn.Module):\n",
        "    def forward(self, y):\n",
        "        size = y.size()\n",
        "        features = y.view(size[0], size[1], size[2] * size[3])\n",
        "        return features.bmm(features.transpose(1, 2)) / (size[1] * size[2] * size[3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b09pX36Bs4Wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InspirationLayer(nn.Module):\n",
        "    def __init__(self, C):\n",
        "        super(InspirationLayer, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.Tensor(1, C, C), requires_grad=True)\n",
        "        self.G = Variable(torch.Tensor(1, C, C), requires_grad=True)\n",
        "        self.C = C\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.weight.data.uniform_(0.0, 0.02)\n",
        "\n",
        "    def setTarget(self, target):\n",
        "        self.G = target\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.P = torch.bmm(self.weight.expand_as(self.G), self.G)\n",
        "        return torch.bmm(self.P.transpose(1, 2).expand(X.size(0), self.C, self.C), X.view(X.size(0), X.size(1), -1)).view_as(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F02ku9VouC8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        self.reflection_padding = nn.ReflectionPad2d(int(np.floor(kernel_size / 2)))\n",
        "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv2d(self.reflection_padding(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vUcbJiouGQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UpConvLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(UpConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        if upsample:\n",
        "            self.upsample_layer = torch.nn.Upsample(scale_factor=upsample)\n",
        "        self.reflection_padding_size = int(np.floor(kernel_size / 2))\n",
        "        if self.reflection_padding_size != 0:\n",
        "            self.reflection_padding = nn.ReflectionPad2d(self.reflection_padding_size)\n",
        "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample:\n",
        "            x = self.upsample_layer(x)\n",
        "        if self.reflection_padding_size != 0:\n",
        "            x = self.reflection_padding(x)\n",
        "        return self.conv2d(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z0OMmQ8uJWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PreResBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n",
        "        super(PreResBlock, self).__init__()\n",
        "        self.expansion = 4\n",
        "        self.downsample = downsample\n",
        "        if self.downsample:\n",
        "            self.residual_layer = nn.Conv2d(inplanes, planes * self.expansion, kernel_size=1, stride=stride)\n",
        "        self.conv_block = nn.Sequential(\n",
        "            norm_layer(inplanes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(inplanes, planes, kernel_size=1, stride=1),\n",
        "            norm_layer(planes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            ConvLayer(planes, planes, kernel_size=3, stride=stride),\n",
        "            norm_layer(planes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(planes, planes * self.expansion, kernel_size=1, stride=1))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.downsample:\n",
        "            residual = self.residual_layer(x)\n",
        "        else:\n",
        "            residual = x\n",
        "        return residual + self.conv_block(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oakg8NpuJde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UpResBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, stride=2, norm_layer=nn.BatchNorm2d):\n",
        "        super(UpResBlock, self).__init__()\n",
        "        self.expansion = 4\n",
        "        self.residual_layer = UpConvLayer(inplanes, planes * self.expansion, kernel_size=1, stride=1, upsample=stride)\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            norm_layer(inplanes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(inplanes, planes, kernel_size=1, stride=1),\n",
        "            norm_layer(planes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            UpConvLayer(planes, planes, kernel_size=3, stride=1, upsample=stride),\n",
        "            norm_layer(planes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(planes, planes * self.expansion, kernel_size=1, stride=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return  self.residual_layer(x) + self.conv_layers(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "467-ASrjrtws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_nc=3, output_nc=3, ngf=64, norm_layer=nn.InstanceNorm2d, n_blocks=6, gpu_ids=[]):\n",
        "        super(Net, self).__init__()\n",
        "        self.gpu_ids = gpu_ids\n",
        "        self.gram = GramMatrix()\n",
        "\n",
        "        block = PreResBlock\n",
        "        upblock = UpResBlock\n",
        "        expansion = 4\n",
        "\n",
        "        self.model1 = nn.Sequential(ConvLayer(input_nc, 64, kernel_size=7, stride=1),\n",
        "                            norm_layer(64),\n",
        "                            nn.ReLU(inplace=True),\n",
        "                            block(64, 32, 2, 1, norm_layer),\n",
        "                            block(128, ngf, 2, 1, norm_layer))\n",
        "\n",
        "        model = []\n",
        "        self.ins = InspirationLayer(ngf * 4)\n",
        "        model += [self.model1]\n",
        "        model += [self.ins]    \n",
        "\n",
        "        for i in range(n_blocks):\n",
        "            model += [block(ngf * 4, ngf, 1, None, norm_layer)]\n",
        "        \n",
        "        model += [upblock(ngf * 4, 32, 2, norm_layer),\n",
        "                            upblock(128, 16, 2, norm_layer),\n",
        "                            norm_layer(64),\n",
        "                            nn.ReLU(inplace=True),\n",
        "                            ConvLayer(64, output_nc, kernel_size=7, stride=1)]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def setTarget(self, Xs):\n",
        "        F = self.model1(Xs)\n",
        "        G = self.gram(F)\n",
        "        self.ins.setTarget(G)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpbbjZOwN6U2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_layer_names = ['conv_4']\n",
        "style_layer_names = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjF0QcKgOJXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Optimizer():\n",
        "    def __init__(self, model):\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def step(self, optimizer_step):\n",
        "        self.optimizer.step(optimizer_step)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEpwaHy0_Wf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(dir, model, epoch_cnt, style_weight, content_weight, cnn, content_layer_names, style_layer_names):\n",
        "    loader = ImageLoader()\n",
        "    loss_values = []\n",
        "    optimizer = Optimizer(model)\n",
        "\n",
        "    style_imgs = []\n",
        "    for style_file in os.listdir(\"./styles/\"):\n",
        "        style_imgs.append(loader.load(\"./styles/\" + style_file))\n",
        "\n",
        "    for epoch in tnrange(epoch_cnt):\n",
        "        def optimizer_step(input_img):\n",
        "            input_img.data.clamp_(0, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            base = BaseModel(cnn, style_img, input_img, content_layer_names, style_layer_names)\n",
        "            base = base.to(device)\n",
        "            style_losses = base.get_style_losses()\n",
        "            content_losses = base.get_content_losses()\n",
        "\n",
        "            base(model(input_img))\n",
        "\n",
        "            style_score = 0\n",
        "            content_score = 0\n",
        "\n",
        "            for sl in style_losses:\n",
        "                style_score += sl.loss\n",
        "            for cl in content_losses:\n",
        "                content_score += cl.loss\n",
        "\n",
        "            loss = style_score * style_weight + content_score * content_weight\n",
        "            loss.backward()\n",
        "            loss_values.append(loss.item())\n",
        "\n",
        "            return loss.item()\n",
        "        \n",
        "        im_cnt = 0\n",
        "        for file in tqdm(os.listdir(\"./train2014/train2014\")):\n",
        "            # im_cnt += 1\n",
        "            # if (im_cnt == 100):\n",
        "            #     break\n",
        "\n",
        "            content_img = loader.load(\"./train2014/train2014/\" + file)\n",
        "            if content_img.shape[1] != 3:\n",
        "                continue\n",
        "\n",
        "            for style_img in style_imgs:\n",
        "                style_img_copy = style_img.clone().detach()\n",
        "                content_img_copy = content_img.clone().detach()\n",
        "                model.setTarget(style_img_copy)                \n",
        "\n",
        "                def opt_step():\n",
        "                    return optimizer_step(content_img_copy)\n",
        "\n",
        "                optimizer.step(opt_step)\n",
        "        \n",
        "        # print(\"Epoch \" + str(epoch))\n",
        "\n",
        "    # im_cnt = 0\n",
        "    # for file in tqdm(os.listdir(\"./train2014/train2014\")):\n",
        "    #     im_cnt += 1\n",
        "    #     if (im_cnt == 10):\n",
        "    #         break\n",
        "    #     input_img = loader.load(\"./train2014/train2014/\" + file)\n",
        "    #     input_img.data.clamp_(0, 1)\n",
        "    #     #   optimizer.zero_grad()\n",
        "    #     #   base = BaseModel(cnn, style_img, input_img, content_layer_names, style_layer_names)\n",
        "    #     #   base = base.to(device)\n",
        "    #     #   output = base(model(input_img))\n",
        "    #     output = model(input_img)\n",
        "    #     imshow(output, title='Image')\n",
        "\n",
        "    return loss_values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcOxM7ux_XHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss_values(loss_values):\n",
        "    plt.plot(np.arange(len(loss_values)), loss_values, color='blue')\n",
        "    plt.title(\"Loss values\")\n",
        "    plt.xlabel(\"iteration\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edjruhp0_W8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net(ngf=64, n_blocks=3)\n",
        "model = model.to(device)\n",
        "epoch_cnt = 2\n",
        "content_weight = 1 # alpha\n",
        "style_weight = 1000000 # beta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iwYaIZ3_XEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_values = fit(\"./train2014/train2014\", model, epoch_cnt, style_weight, content_weight, cnn, content_layer_names, style_layer_names)\n",
        "plot_loss_values(loss_values)\n",
        "\n",
        "loader = ImageLoader()\n",
        "# Images from Gatys\n",
        "style_img = loader.load(\"./data/the-starry-night.jpg\")\n",
        "content_img = loader.load(\"./data/tubingen.jpg\")\n",
        "model.setTarget(style_img)\n",
        "output = model(content_img)\n",
        "\n",
        "imshow(style_img, title='Style Image')\n",
        "imshow(content_img, title='Content Image')\n",
        "imshow(output, title='Image')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPMtl2qk_XRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_values = fit(\"./train2014/train2014\", model, epoch_cnt, style_weight, content_weight, cnn, content_layer_names, style_layer_names)\n",
        "plot_loss_values(loss_values)\n",
        "\n",
        "loader = ImageLoader()\n",
        "# Images from Gatys\n",
        "style_img = loader.load(\"./data/the-starry-night.jpg\")\n",
        "content_img = loader.load(\"./data/tubingen.jpg\")\n",
        "model.setTarget(style_img)\n",
        "output = model(content_img)\n",
        "\n",
        "imshow(style_img, title='Style Image')\n",
        "imshow(content_img, title='Content Image')\n",
        "imshow(output, title='Image')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa0waYKQTDbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}